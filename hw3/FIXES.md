# Исправления в hw3_res

## Выполненные исправления

### Task 1: Проверка корректности алгоритма ✅
**Проблема:** Отсутствовал `#include <algorithm>` для `std::max_element`

**Исправление:**
- Добавлен `#include <algorithm>` в начало файла `task1/src/heat_equation.cpp`
- Обновлен `run_task1.sbatch` для использования `srun` и обработки ошибок модулей

---

### Task 2-4: Масштабируемость ✅
**Проблема:** SLURM не выделял достаточно ресурсов для запуска нескольких процессов

**Исправления:**
- Добавлены директивы в sbatch файлы:
  ```bash
  #SBATCH --ntasks=24
  #SBATCH --cpus-per-task=1
  ```
- Заменен `mpirun` на `srun` для правильной работы с SLURM
- Добавлена обработка ошибок при загрузке модулей

**Измененные файлы:**
- `task2/run_task2.sbatch`
- `task3/run_task3.sbatch`
- `task4/run_task4.sbatch`

---

### Task 5: Гибридная задача (MPI + GPU) ✅
**Проблема:** Модуль `openmpi/4.1.5` не найден, конфликты с MPI из nvhpc

**Исправления:**
- Обновлен `Makefile` для автоматического поиска MPI (из nvhpc или системного)
- Обновлен `run_task5.sbatch`:
  - Убрана загрузка `openmpi/4.1.5` (используется MPI из nvhpc)
  - Добавлена обработка ошибок и поиск MPI в альтернативных путях
  - Использование `srun` вместо `mpirun` для лучшей интеграции с SLURM

---

### Task 6: 2D с MKL ✅
**Проблема:** Модуль `intel/oneapi` не найден

**Исправления:**
- Обновлен `run_task6.sbatch`:
  - Добавлена попытка загрузки нескольких вариантов модулей (`intel/oneapi`, `intel/2021`, `mkl`)
  - Автоматический поиск MKL в стандартных путях
  - Установка `MKLROOT` из переменной окружения или стандартных путей
- Обновлен `Makefile`:
  - Использование переменной окружения `MKLROOT`, если она установлена
  - Автоматический поиск MKL в стандартных путях

---

## Рекомендации для запуска

### Проверка доступных модулей

Перед запуском проверьте доступные модули на вашем кластере:

```bash
# Проверить доступные модули MPI
module avail openmpi
module avail mpi

# Проверить доступные модули Intel/MKL
module avail intel
module avail mkl

# Проверить доступные модули NVIDIA
module avail nvidia
module avail nvhpc
```

### Запуск задач

После исправлений задачи можно запускать:

```bash
# Task 1
cd task1 && sbatch run_task1.sbatch

# Task 2-4 (масштабируемость)
cd task2 && sbatch run_task2.sbatch
cd task3 && sbatch run_task3.sbatch
cd task4 && sbatch run_task4.sbatch

# Task 5 (требует GPU)
cd task5 && sbatch run_task5.sbatch

# Task 6 (требует MKL)
cd task6 && sbatch run_task6.sbatch
```

### Если модули все еще не найдены

1. **Для MPI:** Найдите правильное имя модуля и обновите строку `module load` в соответствующих sbatch файлах
2. **Для MKL:** Если модуль не найден, установите `MKLROOT` вручную перед компиляцией:
   ```bash
   export MKLROOT=/путь/к/mkl
   ```
3. **Для Task 5:** Убедитесь, что MPI доступен из nvhpc или установите системный MPI

---

## Ожидаемые результаты после исправлений

- **Task 1:** Должна успешно скомпилироваться и запуститься
- **Task 2-4:** Должны запускаться с 1, 2, 4, 8, 16, 24 процессами и собирать данные масштабируемости
- **Task 5:** Должна скомпилироваться с MPI+CUDA и запуститься на 2 GPU
- **Task 6:** Должна найти MKL и успешно скомпилироваться

---

## Примечания

- Все исправления обратно совместимы
- Добавлена обработка ошибок для лучшей диагностики проблем
- Использование `srun` вместо `mpirun` обеспечивает лучшую интеграцию с SLURM

