# Домашнее задание №3: Распараллеливание уравнения теплопроводности (MPI)

**Студент:** Макалодин Матвей  
**Курс:** Высокопроизводительные вычисления  
**Дата:** Январь 2025

---

## Обзор проекта

Данная домашняя работа включает реализацию решения одномерного и двумерного уравнений теплопроводности с использованием MPI для параллелизации вычислений. Проект состоит из 6 задач, каждая из которых демонстрирует различные техники параллелизации и оптимизации.

## Структура проекта

```
hw3/
├── task1/          # Проверка корректности алгоритма
├── task2/          # Анализ масштабируемости
├── task3/          # Асинхронная передача данных
├── task4/          # Коллективные операции MPI
├── task5/          # Гибридная задача (MPI + GPU)
├── task6/          # 2D теплопроводность с MKL
├── run_all_tasks.sh    # Скрипт для запуска всех задач
├── analyze_results.py  # Скрипт анализа результатов
└── README.md       # Этот файл
```

## Требования

- **MPI**: OpenMPI 4.1.5 или выше
- **Для Task 5**: CUDA Toolkit, NVIDIA GPU
- **Для Task 6**: Intel MKL библиотека
- **Компилятор**: g++/mpic++ с поддержкой C++17

## Установка и компиляция на кластере

### Загрузка модулей

```bash
module load openmpi/4.1.5
# Для Task 5:
module load nvidia_sdk/nvhpc/23.5
# Для Task 6:
module load intel/oneapi
```

### Компиляция

Для каждой задачи:
```bash
cd task1  # или task2, task3, и т.д.
make
```

## Запуск на кластере

### Автоматический запуск всех задач (рекомендуется)

Используйте скрипт `run_all_tasks.sh` для удобного запуска всех задач:

```bash
# Запустить все задачи
./run_all_tasks.sh

# Запустить только определенные задачи (например, 1, 2, 3)
./run_all_tasks.sh 1 2 3

# Только скомпилировать все задачи без запуска
./run_all_tasks.sh --compile-only

# Запустить без компиляции
./run_all_tasks.sh --no-compile

# Проверить статус запущенных задач
./run_all_tasks.sh --status

# Показать справку
./run_all_tasks.sh --help
```

### Ручной запуск через SLURM

```bash
sbatch run_task1.sbatch
```

### Интерактивно

```bash
mpirun -np 4 ./heat_equation [параметры]
```

## Описание задач

### Task 1: Проверка корректности алгоритма (2 балла)

Реализация параллельной версии алгоритма решения одномерного уравнения теплопроводности с проверкой корректности путем сравнения с точным решением.

**Параметры:**
- `k = 1.0`, `h = 0.02`, `τ = 0.0002`, `T = 0.1`
- Вычисление значений в 11 точках (x = 0, 0.1, 0.2, ..., 1.0)

**Особенности:**
- Сравнение с точным решением
- Вывод статистики ошибок
- Сохранение результатов в файл

### Task 2: Анализ масштабируемости (2 балла)

Исследование масштабируемости параллельной реализации при различных конфигурациях.

**Параметры исследования:**
- Количество процессов: 1, 2, 4, 8, 16, 24
- Количество точек (N): 10000, 25000, 50000
- Конечное время: T = 10⁻⁴

**Результаты:**
- Сохранение времени выполнения в CSV
- Построение графиков масштабируемости

### Task 3: Асинхронная передача данных (2 балла)

Реализация с использованием асинхронных операций MPI (`MPI_Isend`/`MPI_Irecv`) для перекрытия вычислений и передачи данных.

**Особенности:**
- Использование `MPI_Isend` и `MPI_Irecv`
- Перекрытие вычислений внутренних точек с передачей данных
- Сравнение производительности с синхронной версией

### Task 4: Коллективные операции (2 балла)

Использование коллективных операций MPI для распределения начальных данных и сбора результатов.

**Используемые операции:**
- `MPI_Bcast` - распространение информации
- `MPI_Scatterv` - распределение начальных данных
- `MPI_Gatherv` - сбор результатов
- `MPI_Reduce` - сбор статистики

### Task 5: Гибридная задача (1 балл)

Реализация с двумя MPI процессами, каждый из которых выполняет вычисления на своем GPU.

**Особенности:**
- Использование CUDA для вычислений на GPU
- Каждый процесс работает со своим GPU
- Обмен граничными значениями через MPI
- Использование pinned memory

**Примечание:** Цель задачи - изучение компиляции и написания гибридных программ, а не ускорение.

### Task 6: 2D Теплопроводность с MKL (1 балл)

Реализация решения двумерного уравнения теплопроводности с использованием Intel MKL библиотеки.

**Особенности:**
- Решение 2D задачи
- Граничные условия Дирихле
- Использование MKL для эффективных операций с матрицами

## Анализ результатов

Для анализа результатов всех задач используйте скрипт:

```bash
# Установить Python зависимости
pip install -r requirements.txt

# Запустить анализ
python analyze_results.py
```

Скрипт создаст следующие графики:
- **task1_correctness.png** - сравнение численного и точного решений (Task 1)
- **task2_scalability.png** - графики масштабируемости по времени (Task 2)
- **task3_comparison.png** - сравнение синхронной и асинхронной версий (Task 3)
- **task4_performance.png** - производительность с коллективными операциями (Task 4)
- **task6_2d_results.png** - визуализация 2D решения (Task 6)

## Уравнение теплопроводности

### Одномерное уравнение

```
∂u/∂t = k * (∂²u/∂x²)
```

Конечно-разностная схема:
```
u_i^(n+1) = u_i^n + (kτ/h²) * (u_(i+1)^n - 2u_i^n + u_(i-1)^n)
```

Условие устойчивости:
```
kτ/h² < 1  =>  τ < h²/k
```

### Начальные и граничные условия

- **Начальное условие**: u(x,0) = u₀ = 1 для x ∈ [0,1]
- **Граничные условия**: u(0,t) = 0, u(1,t) = 0

### Точное решение (для Task 1)

```
u(x,t) = (4u₀/π) * Σ[m=0 to ∞] (1/(2m+1)) * exp(-kπ²(2m+1)²t/l²) * sin(π(2m+1)x/l)
```

## Параллелизация

### Распределение данных

Весь сегмент делится на `Number Of Proc` частей, так что каждый процессор вычисляет свою часть (`N / Number Of Proc` точек).

### Обмен граничными значениями

При вычислении точек на границах между процессами необходимо использовать передачу граничных значений от соседних процессов для получения значений в соседних точках.

## Файлы для сдачи

Перед сдачей необходимо подготовить:

1. **Исходные коды** - файлы `.cpp` или `.cu` для каждой задачи
2. **Графики анализа** - PDF файл с графиками масштабируемости
3. **README.txt** - описание процесса компиляции и запуска на узлах кластера

## Полезные команды

```bash
# Проверка доступности MPI
mpirun --version

# Запуск с несколькими процессами
mpirun -np 4 ./heat_equation 10000

# Информация о процессах
mpirun -np 4 --display-map ./heat_equation

# Профилирование MPI
mpirun -np 4 --mca pml ob1 --mca btl self,sm ./heat_equation
```

## Контакты

При возникновении вопросов обращайтесь к преподавателю.

